<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Bruce B Campbell]]></title>
  <link href="http://brucebcampbell.github.io/atom.xml" rel="self"/>
  <link href="http://brucebcampbell.github.io/"/>
  <updated>2015-08-02T18:18:59+00:00</updated>
  <id>http://brucebcampbell.github.io/</id>
  <author>
    <name><![CDATA[Bruce Campbell]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Random Matrix Notes]]></title>
    <link href="http://brucebcampbell.github.io/blog/2015/08/02/random-matrix-notes/"/>
    <updated>2015-08-02T18:07:10+00:00</updated>
    <id>http://brucebcampbell.github.io/blog/2015/08/02/random-matrix-notes</id>
    <content type="html"><![CDATA[<p>The classical ensembles of random matrix theory are GOE, GUE, GSE, Wishart, and MANOVA. These correspond to the weight functions of the equilibrium measure of the orthogonal polynomials Hermite, Laguerre,and Jacobi.  The Jacobians of the well known matrix factorizations are used to compute the joint eigenvalue densities of these ensembles. The distribution of eigenvalues of the GOE ensemble follow the well know Winger Semi-circle distribution.
The joint densities up to a constant factor are listed below:
\begin{itemize}
  \item Hermite  \item Laguerre   \item Jacobi
\end{itemize}
We generated histograms in Matlab for samples from the GOE, GUE, GSE, Wishart, and MANOVA ensembles.
The joint PDF of a generic Gaussian ramdom matrix is given by,</p>

<p><script type="math/tex">P(M)=G_\beta(n,m)=\frac{1}{2 \pi^{\frac{\beta n m}{2} }} \exp^{\frac{-1}{2}\norm{M}_F }</script> where <script type="math/tex">\beta</script> encodes the dimension of the field.  Note this leaves open the possibility to
generalize to non integer <script type="math/tex">\beta</script>.</p>

<p>The equations below describes how to generate from the common ensembles starting from a sample <script type="math/tex">A \in G_\beta(n,n)</script></p>

<table>
  <tbody>
    <tr>
      <td>$$GOE = {M</td>
      <td>M = \frac{A+A^T}{2}, A \in G_1(n,n)}$$</td>
    </tr>
    <tr>
      <td>$$GUE = {M</td>
      <td>M = \frac{A+A^\dagger}{2}, A \in G_2(n,n)}$$</td>
    </tr>
    <tr>
      <td>$$GSE = {M</td>
      <td>M = \frac{A+A^\ddagger}{2}, A \in G_4(n,n)}$$</td>
    </tr>
  </tbody>
</table>

<p>The <script type="math/tex">CS</script> decomposition is a matrix factorization equivalent to four <script type="math/tex">SVD</script>’s which correspond to rotation problems
$$\left(\begin{array}{cc}
        X \rightarrow  Y &amp; X^\perp \rightarrow  Y <br />
        X \rightarrow  Y^\perp &amp; X^\perp \rightarrow  Y^\perp <br />
\end{array}\right)$$</p>

<p>Which can be compactly written
$$[X | X^\perp]^T [Y | Y^\perp ]=\left(
      \begin{array}{cc}
        Q<em>{11} &amp; Q</em>{12} <br />
        Q<em>{21} &amp; Q</em>{22} <br />
      \end{array}
\right)$$</p>

<script type="math/tex; mode=display">% &lt;![CDATA[
\left(
      \begin{array}{cc}
        Q_{11} & Q_{12} \\
        Q_{21} & Q_{22} \\
      \end{array}
\right)    = \left(
      \begin{array}{cc}
        U_1 & 0 \\
        0 & U_2
      \end{array}
\right) * \left(
      \begin{array}{cc}
        C & S \\
        -S & C
      \end{array}
\right) * \left(
      \begin{array}{cc}
        V_1 & 0 \\
        0 & V_2
      \end{array}
\right)
 %]]&gt;</script>

<p>Where U, S are unary.</p>

<p>The Tracy-Widom law of order one is the limiting distribution of the largest eigenvalue of a Wishart matrix with identity covariance when properly scaled.  This has some application to weighted directional graphs.  The largest eigenvalue of the adjacency matrix of a random d-regular directed graph follows the Tracy-Widom law.  The kernels of integrable operators describe the asymptotic eigenvalue distribution of self-adjoint random matrices from the unitary ensembles. Consider the discreet operator <script type="math/tex">K(n,m):  \l^2(N) \rightarrow \l^2(M)</script> where <script type="math/tex">% &lt;![CDATA[
K(n,m) = \frac{(<J a(m),a(n)>}{m-n} %]]&gt;</script> the discrete Bessel kernel and kernels arising from the almost Mathieu equation.  The celebrated paper of Tracy and Widom investigated integral kernels of the form
<script type="math/tex">K(x,y)=\frac{f(x)g(y)-f(y)g(x)}{x-y} : x \neq y  f(x), g(x) \in L^2(0,\infty)</script> are solutions to the system of <script type="math/tex">ODE</script>’s</p>

<script type="math/tex; mode=display">% &lt;![CDATA[

\frac{d}{dx}\left( \begin{array}{c}
        f(x) \\
        g(x)
      \end{array}
\right) = \left(
      \begin{array}{cc}
        \alpha(x) & \beta(x) \\
        -\gamma(x) & -\alpha(x)
      \end{array}
\right) * \left( \begin{array}{c}
        f(x) \\
        g(x)
      \end{array} \right)
 %]]&gt;</script>

<p>Let <script type="math/tex">\phi_i(x)</script> be an orthogonal basis in a Hilbert Space <script type="math/tex">\mathcal{H}</script> where
$$
\Gamma<em>{\phi}
= {\phi</em>(j+k-1)}_{j,k=1}^{\infty}
$$ is the induced Hankel Matrix.</p>

<p>Let <script type="math/tex">\mathcal(L) : \mathcal{H} \rightarrow \mathcal{H}</script> be compact.  Then <script type="math/tex">% &lt;![CDATA[
\mathcal(L): f \mapsto \sum\limits_{n=1}^{N} \omega_n <\phi_n, f> \psi_n %]]&gt;</script> where <script type="math/tex">\{\phi_i\}_{i=1}^{N}</script></p>

<p>The Tracy-Widom distribution is related to to determinantal stochastic processes.  A process following this law is distributed as the largest point of a point process on the real line where the kernel K is the so-called Airy kernel.  In addition to describing the edge spectrum of random matrices, it arises in several place in combinatorial for instance the longest increasing subsequences of random permutations is described by the Tracy Widom law.  In addition to the eigenvalues of random matrices, this type of point process is used in models such as  fluctuations in first and last passage percolation, and the asymmetric exclusion process.</p>

<p>The path configuration of random viscous walkers is related to the Young tableaux.  Statistical problems related to the Young tableaux include random growth, point processes, random permutation, and the random word problem. the asymptotic distribution of scaled variables from these models are  described by the Tracy Widom distribution which is the limit distribution for the largest eigenvalue of <script type="math/tex">X \in GUE</script>.</p>

<p>C A Tracy and H Widom,Correlation functions, cluster functionsand spacing distribution for random matrices,
J. Statist. Phys 1998, 92</p>

<p>A Soshnikov, A note on universality of the distribution of the largest eigenvalues in certain sample covariance matrices
J. Statist. Phys 2002, 108, p1033–1056</p>

<p>D Achlioptas, Random matrices in data analysis
Proceedings of the 15 th European Conference on Machine Learning,2004,p1–8</p>

<p>N Alon and M Krivelevich and V H Vu, On the concentration of eigenvalues of random symmetric matrices,
Israel J. Math 2000</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Analytic Learning Theory]]></title>
    <link href="http://brucebcampbell.github.io/blog/2015/07/27/analytic-learning-theory/"/>
    <updated>2015-07-27T22:40:08+00:00</updated>
    <id>http://brucebcampbell.github.io/blog/2015/07/27/analytic-learning-theory</id>
    <content type="html"><![CDATA[<p>This post introduces the language of analytic learning theory. </p>

<p>Supervised learning in its most abstract setting requires finding a function $f(x)$ given instances ${ (x_i ,f(x_i))}$. Typically ${x_i}$ is an independent and identically distribute (iid) sample from some unknown distribution.  A loss function is a random variable <script type="math/tex">L : Ran(f) \times Ran(f) \rightarrow \mathcal{R}^+</script> defining the cost of misclassification.  The risk associated with a candidate function $f’$ is defined to be the expectation of the loss over the sample space $\Omega$ given that the true functional relationship is $(x,f(x))$</p>

<script type="math/tex; mode=display">R(f') =: E(L(f')) =  \int L( f(\omega), f'(\omega)) d\omega</script>

<p>Statistical learning theory is concerned with assessing the approximations to $f$ given by minimizing the empirical loss associated with a sample ${x_i ,f(x_i)}$.</p>

<p>It’s worth noting that minimization of expected loss is not always the best things to do.  To understand this we must make the distinction between loss and utility.  The St. Petersburg paradox is an example of a random variable $S : \mathcal{N} \rightarrow \mathcal{R+}$ with infinite expectation but limited utility. Let $W(k)$ be the winnings after k plays of from a game with outcome $S$ that pays $2^{i-1}$ with probability $p_i=1/2^i$. The expected payout is given by</p>

<p><script type="math/tex">\lim_{k \rightarrow \infty} W(k)/k =E(S)=\sum\limits_{i=1}^{\infty} p_i 2^{i-1}= \infty</script> 
The implication for a decision theory based only on expected value is that a rational player would pay an infinite amount of money to play this game. Bernoulli introduced the notion of expected utility which takes into account the fact that a payout of $2^i$ may not have twice the utility of a payout of $2^{i+1}$ when $i$ gets large.  The utility $U$ is a random variable on the sample space representing preferences of an agent.  Loss represents the aversion of an agent to the outcomes of the sample space, $L(\omega)+U(\omega) = \alpha \; \forall \omega \in \Omega$ where $\alpha$ is constant.  Expected loss $R(f’)$ is the risk associated with choosing the approximation $f’$. Restricting the class of functions to consider when minimizing the risk for a candidate approximation to $f$ is a key aspect of classifier design</p>

<p>The most common loss function is the $L^2$ or squared error loss.  This has a number of convenient numerical and statistical properties. Other common loss functions are the $0-1$ loss and the $L^1$  or sparse loss.  There is a vast literature on $L^1$ (sparse) methods in machine learning since one is always interested in a sparse representation of a signal.</p>

<p>Vladimir N. Vapnik. The nature of statistical learning theory, 1995.</p>

<p>Felipe Cucker and Steve Smale. On the mathematical foundations of learning.
Bulletin of the American Mathematical Society, 39:1 49, 2002.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Notes Large Scale Linear Algebra]]></title>
    <link href="http://brucebcampbell.github.io/blog/2015/07/10/notes-large-scale-linear-algebra/"/>
    <updated>2015-07-10T21:35:03+00:00</updated>
    <id>http://brucebcampbell.github.io/blog/2015/07/10/notes-large-scale-linear-algebra</id>
    <content type="html"><![CDATA[
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Support Vector Machines]]></title>
    <link href="http://brucebcampbell.github.io/blog/2015/05/30/multiclasssvm/"/>
    <updated>2015-05-30T23:22:44+00:00</updated>
    <id>http://brucebcampbell.github.io/blog/2015/05/30/multiclasssvm</id>
    <content type="html"><![CDATA[<p>Below is the class declaration from the multiclass Support Vector Machine implemented in klMatrix.
This uses the Sequential minimal optimization (SMO) algorithm for solving the quadratic programming (QP) 
problem that arises during the training of the SVM.  The algorithm implements the work of 
[Borer, Silvio. New support vector algorithms for multicategorical data applied to real-time object recognition. Diss. Ecole Polytechnique Federale de Lausanne, 2003].</p>

<div class="bogus-wrapper"><notextile><figure class="code"><div class="highlight"><table><tr><td class="gutter"><pre class="line-numbers"><span class="line-number">1</span>
<span class="line-number">2</span>
<span class="line-number">3</span>
<span class="line-number">4</span>
<span class="line-number">5</span>
<span class="line-number">6</span>
<span class="line-number">7</span>
<span class="line-number">8</span>
<span class="line-number">9</span>
<span class="line-number">10</span>
<span class="line-number">11</span>
<span class="line-number">12</span>
<span class="line-number">13</span>
<span class="line-number">14</span>
<span class="line-number">15</span>
<span class="line-number">16</span>
<span class="line-number">17</span>
<span class="line-number">18</span>
<span class="line-number">19</span>
<span class="line-number">20</span>
<span class="line-number">21</span>
<span class="line-number">22</span>
<span class="line-number">23</span>
<span class="line-number">24</span>
<span class="line-number">25</span>
<span class="line-number">26</span>
<span class="line-number">27</span>
<span class="line-number">28</span>
<span class="line-number">29</span>
<span class="line-number">30</span>
<span class="line-number">31</span>
<span class="line-number">32</span>
<span class="line-number">33</span>
<span class="line-number">34</span>
<span class="line-number">35</span>
<span class="line-number">36</span>
<span class="line-number">37</span>
<span class="line-number">38</span>
<span class="line-number">39</span>
<span class="line-number">40</span>
<span class="line-number">41</span>
<span class="line-number">42</span>
<span class="line-number">43</span>
<span class="line-number">44</span>
<span class="line-number">45</span>
</pre></td><td class="code"><pre><code class=""><span class="line">template &lt;class TYPE&gt; class klMulticlassSVMTrain : public klSamplePopulation&lt;TYPE&gt;
</span><span class="line">{
</span><span class="line">public:
</span><span class="line">    //These are the outputs of the training operation
</span><span class="line">    klVector&lt;TYPE&gt; _lagMults;
</span><span class="line">    klVector&lt;TYPE&gt; _intercepts;
</span><span class="line">    klMatrix&lt;TYPE&gt; _supportPoints;
</span><span class="line">    klVector&lt;int&gt;  _supportClasses;
</span><span class="line">
</span><span class="line">
</span><span class="line">    //Inputs of the training operation
</span><span class="line">    klVector&lt;int&gt; _trainingClasses;
</span><span class="line">
</span><span class="line">    //	  nu                - fraction of the distribution to exclude - called
</span><span class="line">    //                     "outlierFraction" in much of the rest of the SVM code
</span><span class="line">    //						p x 1 array of desired fraction of outliers, one for each of p classes.
</span><span class="line">    klVector&lt;TYPE&gt; _outlierFraction;
</span><span class="line">
</span><span class="line">    //  mixingCoeff =0.5 is a good place to start
</span><span class="line">    TYPE _mixingCoeff;
</span><span class="line">
</span><span class="line">    unsigned int _numClasses;
</span><span class="line">
</span><span class="line">    //	double smoThresh=1/10,000; is the  tolerance for SMO algorithm
</span><span class="line">    TYPE _smoThresh;
</span><span class="line">
</span><span class="line">
</span><span class="line">    //The width of RBF kernel.
</span><span class="line">    double _sigma;
</span><span class="line">
</span><span class="line">    TYPE RadialBasisFunctionKernel(TYPE* x, TYPE* y) 
</span><span class="line">    {
</span><span class="line">        unsigned int     n;
</span><span class="line">        TYPE  sum, d;
</span><span class="line">
</span><span class="line">        sum = 0;
</span><span class="line">        for(n = 0; n &lt;getColumns(); n++)
</span><span class="line">        {
</span><span class="line">            d = x[n] - y[n];
</span><span class="line">            sum += d * d;
</span><span class="line">        }
</span><span class="line">
</span><span class="line">        return(std::exp(-   sum /(2.0 * _sigma * _sigma)) );  
</span><span class="line">    }
</span><span class="line"> ...</span></code></pre></td></tr></table></div></figure></notextile></div>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Can You Hear the Shape of a Drum - Spectral Geometry]]></title>
    <link href="http://brucebcampbell.github.io/blog/2014/03/09/CanYouHearTheShapeOfADrum/"/>
    <updated>2014-03-09T23:44:50+00:00</updated>
    <id>http://brucebcampbell.github.io/blog/2014/03/09/CanYouHearTheShapeOfADrum</id>
    <content type="html"><![CDATA[<p>Spectral Geometry concerns itself with the relationships between a geometric structure and the spectra of a differential operator, typically the Laplacian.   Inferring the geometry from the spectra is a type of inverse problem since two non isometric manifolds may share the same spectra.  Going the other way, we encounter isoperimetric inequalities and spectral gap theorems.  “Can One Hear the Shape of a Drum?” was the title of an article  by Mark Kac in the American Mathematical Monthly 1966.   The frequencies at which a drum vibrate depends on its shape.  </p>

<p>The elliptic PDE  <script type="math/tex"> \nabla^2 A + k A = 0</script> tells us the frequencies if we know the shape.</p>

<p>These frequencies are the eigenvalues of the Laplacian in the region.  Can the spectrum of the Laplacian  tell us the shape if we know the frequencies?  Hermann Weyl showed the eigenvalues  of the Laplacian in the compact domain <script type="math/tex">\Omega</script> are distributed according to</p>

<script type="math/tex; mode=display"> N(\lambda) \sim (2 \pi)^{-d) \omega_d \lambda^{\frac{d}{2}} vol(\Omega}</script>

<p>The Laplace Beltrami operator is the generalization of <script type="math/tex">\nabla \circ \nabla = \Delta</script> to <script type="math/tex">\mathcal{M}</script> </p>

<script type="math/tex; mode=display">\Delta f = tr(H(f))</script>

<p>In the exterior calculus we have <script type="math/tex"> \Delta f = d^*d \; f</script>.</p>

<p>The Laplacian of a Gaussian has well known applications in image processing.
Given <script type="math/tex">f(x,y)</script>, we get a scale space representation when we convolve by</p>

<script type="math/tex; mode=display">g(x,y,t) = \frac{e^{x^2+y^2}}{2 \pi t}</script>

<script type="math/tex; mode=display">L(x,y,t) =g(x,y,t) \ast f(x,y)</script>

<p>Applying <script type="math/tex">\Delta</script> to <script type="math/tex">L(x,y,t)</script> gives response to blobs of extent <script type="math/tex">\sqrt{t}</script></p>

<p>There is a well known connection between diffusion processes and Schrodinger operators;</p>

<script type="math/tex; mode=display"> H = \nabla^2 + V(x) \Phi \in L^2(R^n) </script>

<script type="math/tex; mode=display">H \Phi = E \Phi </script>

<script type="math/tex; mode=display">E = \sigma(H) </script>

]]></content>
  </entry>
  
</feed>
